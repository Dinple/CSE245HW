{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Sequential, Linear, ReLU, Softmax\n",
    "from torch.nn import functional as F\n",
    "from scipy.sparse import random\n",
    "from scipy import stats\n",
    "from numpy.random import default_rng\n",
    "\n",
    "from gmm import GMM\n",
    "# circuit-sim\n",
    "# import subcircuit as sc\n",
    "# import PySpice.Logging.Logging as Logging\n",
    "# logger = Logging.setup_logging()\n",
    "# from PySpice.Spice.Netlist import Circuit, SubCircuit, SubCircuitFactory\n",
    "# from PySpice.Unit import *\n",
    "# from PySpice.Spice.NgSpice.Shared import NgSpiceShared\n",
    "# mps_device = torch.device(\"mps\")\n",
    "mps_device = torch.device(\"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Type | Value                          | Type | Value                              |\n",
    "|------|--------------------------------|------|------------------------------------|\n",
    "| Node | capacitance value              | Path | input transition time              |\n",
    "|      | num of input nodes             |      | drive strength of drive cell       |\n",
    "|      | total input cap                |      | functionality  of drive cell       |\n",
    "|      | total output cap               |      | drive strength of load cell        |\n",
    "|      | number of connected resistance |      | functionality of load cell         |\n",
    "|      | total input resistance         |      | effective capacitance of load cell |\n",
    "|      | total output resistance        |      | wire path Elmore delay             |\n",
    "|      | ~~Elmore downstream capacitance~~  |      | wire path D2M delay                |\n",
    "|      | ~~Elmore stage delay~~             |      |                                    |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_i^{l_1} = ReLU(W_1^{l_1}x_i^{l_1-1}+W_2^{l_1}a_{iu}\\sum_{u\\in \\mathcal{N}(v_i)}x_u^{l_1-1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "# torch random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "class L1_GNNModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified GraphSAGE module with weighted aggregation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_output_features):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Parameter(\n",
    "            torch.randn(num_output_features, num_output_features)\n",
    "        ).to(mps_device)\n",
    "        self.W2 = nn.Parameter(\n",
    "            torch.randn(num_output_features, num_output_features)\n",
    "        ).to(mps_device)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.W2, gain=1.414).to(mps_device)\n",
    "        nn.init.xavier_uniform_(self.W1, gain=1.414).to(mps_device)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        neighbors_agg = (\n",
    "            to_dense_adj(edge_index, edge_attr=edge_weight).squeeze(0).to(mps_device)\n",
    "        )\n",
    "        neighbors_agg = torch.matmul(\n",
    "            neighbors_agg.float(), x.float().to(mps_device)\n",
    "        ).to(mps_device)\n",
    "        # linear transformation\n",
    "        out = torch.matmul(\n",
    "            neighbors_agg.to(torch.float32).to(mps_device), self.W2.to(torch.float32).to(mps_device)\n",
    "        ) + torch.matmul(x.to(torch.float32).to(mps_device), self.W1.to(torch.float32).to(mps_device))\n",
    "        # relu activation\n",
    "        # out = torch.relu(out)\n",
    "        # use leaky relu\n",
    "        out = F.leaky_relu(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\tilde{a}_{iu}^{(k, l_2)} = softmax(\\frac{W_Q^{(k, l_2)}x_i^{(L_1+l_2-1)}(W_k^{(k,l_2)}x_u^{(L_1+l_2-1)})}{\\sqrt{d_k}})$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_i^{(L_1+l_2)} = x_i^{(L_1+l_2-1)} +  W_3^{(l_2)}||^{\\mathcal{K}}_{k=1}\\sum_{u\\in \\mathcal{V}}\\tilde{a}_{iu}^{(k, l_2)}(W_V^{(k, l_2)}x_u^{(L_1+l_2-1)})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2_GNNModule(nn.Module):\n",
    "    \"\"\"Multi-head Attention GNN module\n",
    "    \"\"\"\n",
    "    def __init__(self, num_output_features, num_of_heads=4):\n",
    "        super().__init__()\n",
    "        self.Wq = nn.Parameter(torch.randn(num_of_heads, num_output_features, num_output_features)).to(torch.float32).to(mps_device)\n",
    "        self.Wk = nn.Parameter(torch.randn(num_of_heads, num_output_features, num_output_features)).to(torch.float32).to(mps_device)\n",
    "        self.Wv = nn.Parameter(torch.randn(num_of_heads, num_output_features, num_output_features)).to(torch.float32).to(mps_device)\n",
    "        self.W3 = nn.Parameter(torch.randn(num_output_features, num_output_features)).to(torch.float32).to(mps_device)\n",
    "        self.num_of_heads = num_of_heads\n",
    "        self.num_output_features = num_output_features\n",
    "\n",
    "        nn.init.xavier_uniform_(self.Wq, gain=1.414).to(mps_device)\n",
    "        nn.init.xavier_uniform_(self.Wk, gain=1.414).to(mps_device)\n",
    "        nn.init.xavier_uniform_(self.Wv, gain=1.414).to(mps_device)\n",
    "        nn.init.xavier_uniform_(self.W3, gain=1.414).to(mps_device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # self-attention map A\n",
    "        x = x.to(torch.float32)\n",
    "        q = torch.matmul(x, self.Wq)\n",
    "        q = q.view(-1, self.num_of_heads, self.num_output_features)\n",
    "        k = torch.matmul(x, self.Wk)\n",
    "        k = k.view(-1, self.num_of_heads, self.num_output_features)\n",
    "        v = torch.matmul(x, self.Wv)\n",
    "        v = v.view(-1, self.num_of_heads, self.num_output_features)\n",
    "\n",
    "        # transpose to num_of_heads * num_nodes * num_node_features\n",
    "        q = q.transpose(0,1)\n",
    "        k = k.transpose(0,1)\n",
    "        v = v.transpose(0,1)\n",
    "\n",
    "        d_k = torch.tensor(self.num_output_features, dtype=torch.float)\n",
    "        a_iu = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(d_k)\n",
    "        a_iu = torch.softmax(a_iu, dim=-1)\n",
    "        a_iu = torch.matmul(a_iu, v)\n",
    "\n",
    "        # concat heads\n",
    "        concat = a_iu.contiguous().view(self.num_of_heads, -1, self.num_output_features)\n",
    "        \n",
    "        # reshape to num_nodes * num_node_features by averaging over heads\n",
    "        concat = concat.transpose(0,1)\n",
    "        concat = concat.mean(dim=1)\n",
    "        l2 = torch.matmul(concat, self.W3)\n",
    "        x = x + l2\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNtrans(nn.Module):\n",
    "    def __init__(self, num_node_features, num_path_features) -> None:\n",
    "        super().__init__()\n",
    "        self.num_node_features = num_node_features\n",
    "        self.num_path_features = num_path_features\n",
    "        self.gnn1 = L1_GNNModule(num_node_features)\n",
    "        # self.gnn2 = L1_GNNModule(num_node_features)\n",
    "        # self.gnn3 = L1_GNNModule(num_node_features)\n",
    "        # self.gnn4 = L1_GNNModule(num_node_features)\n",
    "        # self.gnn5 = L1_GNNModule(num_node_features)\n",
    "        self.trans1 = L2_GNNModule(num_node_features, num_of_heads=4)\n",
    "        # self.trans2 = L2_GNNModule(num_node_features, num_of_heads=4)\n",
    "        # MLP layer predicting slew and delay\n",
    "        self.linear_slew = nn.Linear(num_node_features + num_path_features, 1)\n",
    "        self.linear_delay = nn.Linear(num_node_features + num_path_features + 1, 1)\n",
    "        # initialize weights\n",
    "        nn.init.xavier_uniform_(self.linear_slew.weight, gain=1.414).to(mps_device)\n",
    "        nn.init.xavier_uniform_(self.linear_delay.weight, gain=1.414).to(mps_device)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight, path_indices, path_features):\n",
    "        x_1 = self.gnn1(x, edge_index, edge_weight)\n",
    "        # x_1 = self.gnn2(x_1, edge_index, edge_weight)\n",
    "        # x_1 = self.gnn3(x_1, edge_index, edge_weight)\n",
    "        # x_1 = self.gnn4(x_1, edge_index, edge_weight)\n",
    "        # x_1 = self.gnn5(x_1, edge_index, edge_weight)\n",
    "        x_2 = self.trans1(x_1)\n",
    "        # x_2 = self.trans2(x_2)\n",
    "        \n",
    "        # path aggregation\n",
    "        y = []\n",
    "        for path_index, path_feature in zip(path_indices, path_features):\n",
    "            # select node based on path index\n",
    "            x_path = torch.index_select(x_2, 0, path_index)\n",
    "            # average pooling based on each path\n",
    "            x_path = torch.mean(x_path, dim=0)\n",
    "            # concatenate with path features\n",
    "            x_path = torch.cat((x_path, path_feature))\n",
    "            # add a dimension\n",
    "            x_path = x_path.unsqueeze(0)\n",
    "            # append to output\n",
    "            y.append(x_path[0])\n",
    "        \n",
    "        # convert to tensor\n",
    "        y = torch.stack(y)\n",
    "        # predict slew\n",
    "        slew = self.linear_slew(y)\n",
    "        # concatenate slew with path features\n",
    "        y = torch.cat((y, slew), dim=1)\n",
    "        # predict delay\n",
    "        delay = self.linear_delay(y)\n",
    "\n",
    "        return slew, delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''Test the GNN'''\n",
    "# # input features from node attributes\n",
    "# x = torch.tensor([[G.nodes[i]['capacitance'],\n",
    "#                     G.nodes[i]['num_input_nodes'],\n",
    "#                     G.nodes[i]['input_cap'],\n",
    "#                     G.nodes[i]['output_cap'],\n",
    "#                     G.nodes[i]['connected_resistance'],\n",
    "#                     G.nodes[i]['input_resistance'],\n",
    "#                     G.nodes[i]['output_resistance'],\n",
    "#                     G.nodes[i]['elmore_cap'],\n",
    "#                     G.nodes[i]['elmore_delay']] for i in range(num_nodes)], dtype=torch.float)\n",
    "\n",
    "# # extract edge index from adjacency matrix from networkx\n",
    "# edge_index = torch.tensor([[i, j] for i in range(num_nodes) for j in range(num_nodes) if M_adj.A[i,j] != 0], dtype=torch.long).t()\n",
    "\n",
    "# print(edge_index)\n",
    "\n",
    "# # extract edge weights from networkx\n",
    "# edge_weight = torch.tensor([G.edges[i,j]['weight'] for i in range(num_nodes) for j in range(num_nodes) if M_adj.A[i,j] != 0], dtype=torch.float)\n",
    "\n",
    "# # extract path indices from networkx : node along the path\n",
    "# all_path_indices = torch.tensor(list_of_paths, dtype=torch.long)\n",
    "\n",
    "# # extract path features from networkx\n",
    "# path_features = M_path\n",
    "                                \n",
    "# num_path_features = path_features.shape[1]\n",
    "\n",
    "# gnn = GNNtrans(num_node_features, num_node_features, num_path_features)\n",
    "# slew, delay = gnn(x, edge_index, edge_weight, all_path_indices, path_features)\n",
    "\n",
    "# slew_target = 0.1\n",
    "# slew_target = torch.tensor(slew_target, dtype=torch.float)\n",
    "# delay_target = 0.2\n",
    "# delay_target = torch.tensor(delay_target, dtype=torch.float)\n",
    "\n",
    "# print(\"Slew: \", slew)\n",
    "# print(\"Slew Shape: \", slew.shape)\n",
    "\n",
    "# print(\"Delay: \", delay)\n",
    "# print(\"Delay Shape: \", delay.shape)\n",
    "\n",
    "# print(\"x Shape: \", x.shape)\n",
    "# print(\"edge_index Shape: \", edge_index.shape)\n",
    "# print(\"edge_weight Shape: \", edge_weight.shape)\n",
    "# print(\"path_indices Shape: \", all_path_indices.shape)\n",
    "# print(\"path_features Shape: \", path_features.shape)\n",
    "# print(\"slew_target Shape: \", slew_target.shape)\n",
    "# print(\"delay_target Shape: \", delay_target.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np \n",
    "# import os\n",
    "\n",
    "# train_size = 8000\n",
    "# test_size = 2000\n",
    "\n",
    "# # extract sample from path\n",
    "# SAMPLE_DIR = \"./sample/Data/\"\n",
    "# TRAIN_DIR = \"./sample/Train/\"\n",
    "# TEST_DIR = \"./sample/Test/\"\n",
    "\n",
    "# # 8 stage\n",
    "# label_delay_stage8 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage8_label_delay.npy\"))\n",
    "# label_delay_stage8 = torch.from_numpy(label_delay_stage8)\n",
    "# label_delay_stage8 = label_delay_stage8.to(torch.float32).to(mps_device)\n",
    "# label_slew_stage8 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage8_label_slew.npy\"))\n",
    "# label_slew_stage8 = torch.from_numpy(label_slew_stage8)\n",
    "# label_slew_stage8 = label_slew_stage8.to(torch.float32).to(mps_device)\n",
    "# all_node_feature_stage8 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage8_node_features.npy\"))\n",
    "# all_node_feature_stage8 = torch.from_numpy(all_node_feature_stage8).to(torch.float32).to(mps_device)\n",
    "# all_path_feature_stage8 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage8_path_features.npy\"))\n",
    "# all_path_feature_stage8 = np.expand_dims(all_path_feature_stage8, axis=1)\n",
    "# all_path_feature_stage8 = torch.from_numpy(all_path_feature_stage8).to(torch.float32).to(mps_device)\n",
    "# all_edge_index_stage8 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage8_edge_index.npy\"))\n",
    "# all_edge_index_stage8 = torch.from_numpy(all_edge_index_stage8).to(torch.long).to(mps_device)\n",
    "# all_edge_weight_stage8 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage8_edge_weight.npy\"))\n",
    "# all_edge_weight_stage8 = torch.from_numpy(all_edge_weight_stage8).to(torch.float32).to(mps_device)\n",
    "# all_path_indices_stage8 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage8_path_indices.npy\"))\n",
    "# all_path_indices_stage8 = torch.from_numpy(all_path_indices_stage8).to(torch.long).to(mps_device)\n",
    "\n",
    "# print(\"label_delay_stage8: \", label_delay_stage8.shape)\n",
    "# print(\"label_slew_stage8: \", label_slew_stage8.shape)\n",
    "# print(\"all_node_feature_stage8: \", all_node_feature_stage8.shape)\n",
    "# print(\"all_path_feature_stage8: \", all_path_feature_stage8.shape)\n",
    "# print(\"all_edge_index_stage8: \", all_edge_index_stage8.shape)\n",
    "# print(\"all_edge_weight_stage8: \", all_edge_weight_stage8.shape)\n",
    "# print(\"all_path_indices_stage8: \", all_path_indices_stage8.shape)\n",
    "# print(\"---------------------------------\")\n",
    "\n",
    "# # train test split 8 stage\n",
    "# train_delay_stage8, test_delay_stage8 = torch.utils.data.random_split(label_delay_stage8, [train_size, test_size])\n",
    "# train_slew_stage8, test_slew_stage8 = torch.utils.data.random_split(label_slew_stage8, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_node_feature_stage8, test_node_feature_stage8 = torch.utils.data.random_split(all_node_feature_stage8, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_path_feature_stage8, test_path_feature_stage8 = torch.utils.data.random_split(all_path_feature_stage8, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_edge_index_stage8, test_edge_index_stage8 = torch.utils.data.random_split(all_edge_index_stage8, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_edge_weight_stage8, test_edge_weight_stage8 = torch.utils.data.random_split(all_edge_weight_stage8, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_path_indices_stage8, test_path_indices_stage8 = torch.utils.data.random_split(all_path_indices_stage8, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# # save train test split 8 stage with pytorch\n",
    "# torch.save(train_delay_stage8, os.path.join(TRAIN_DIR, \"train_delay_stage8.pt\"))\n",
    "# torch.save(train_slew_stage8, os.path.join(TRAIN_DIR, \"train_slew_stage8.pt\"))\n",
    "# torch.save(train_node_feature_stage8, os.path.join(TRAIN_DIR, \"train_node_feature_stage8.pt\"))\n",
    "# torch.save(train_path_feature_stage8, os.path.join(TRAIN_DIR, \"train_path_feature_stage8.pt\"))\n",
    "# torch.save(train_edge_index_stage8, os.path.join(TRAIN_DIR, \"train_edge_index_stage8.pt\"))\n",
    "# torch.save(train_edge_weight_stage8, os.path.join(TRAIN_DIR, \"train_edge_weight_stage8.pt\"))\n",
    "# torch.save(train_path_indices_stage8, os.path.join(TRAIN_DIR, \"train_path_indices_stage8.pt\"))\n",
    "# torch.save(test_delay_stage8, os.path.join(TEST_DIR, \"test_delay_stage8.pt\"))\n",
    "# torch.save(test_slew_stage8, os.path.join(TEST_DIR, \"test_slew_stage8.pt\"))\n",
    "# torch.save(test_node_feature_stage8, os.path.join(TEST_DIR, \"test_node_feature_stage8.pt\"))\n",
    "# torch.save(test_path_feature_stage8, os.path.join(TEST_DIR, \"test_path_feature_stage8.pt\"))\n",
    "# torch.save(test_edge_index_stage8, os.path.join(TEST_DIR, \"test_edge_index_stage8.pt\"))\n",
    "# torch.save(test_edge_weight_stage8, os.path.join(TEST_DIR, \"test_edge_weight_stage8.pt\"))\n",
    "# torch.save(test_path_indices_stage8, os.path.join(TEST_DIR, \"test_path_indices_stage8.pt\"))\n",
    "\n",
    "# # 6 stage\n",
    "# label_delay_stage6 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage6_label_delay.npy\"))\n",
    "# label_delay_stage6 = torch.from_numpy(label_delay_stage6)\n",
    "# label_delay_stage6 = label_delay_stage6.to(torch.float32).to(mps_device)\n",
    "# label_slew_stage6 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage6_label_slew.npy\"))\n",
    "# label_slew_stage6 = torch.from_numpy(label_slew_stage6)\n",
    "# label_slew_stage6 = label_slew_stage6.to(torch.float32).to(mps_device)\n",
    "# all_node_feature_stage6 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage6_node_features.npy\"))\n",
    "# all_node_feature_stage6 = torch.from_numpy(all_node_feature_stage6).to(torch.float32).to(mps_device)\n",
    "# all_path_feature_stage6 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage6_path_features.npy\"))\n",
    "# all_path_feature_stage6 = np.expand_dims(all_path_feature_stage6, axis=1)\n",
    "# all_path_feature_stage6 = torch.from_numpy(all_path_feature_stage6).to(torch.float32).to(mps_device)\n",
    "# all_edge_index_stage6 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage6_edge_index.npy\"))\n",
    "# all_edge_index_stage6 = torch.from_numpy(all_edge_index_stage6).to(torch.long).to(mps_device)\n",
    "# all_edge_weight_stage6 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage6_edge_weight.npy\"))\n",
    "# all_edge_weight_stage6 = torch.from_numpy(all_edge_weight_stage6).to(torch.float32).to(mps_device)\n",
    "# all_path_indices_stage6 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage6_path_indices.npy\"))\n",
    "# all_path_indices_stage6 = torch.from_numpy(all_path_indices_stage6).to(torch.long).to(mps_device)\n",
    "\n",
    "# print(\"label_delay_stage6: \", label_delay_stage6.shape)\n",
    "# print(\"label_slew_stage6: \", label_slew_stage6.shape)\n",
    "# print(\"all_node_feature_stage6: \", all_node_feature_stage6.shape)\n",
    "# print(\"all_path_feature_stage6: \", all_path_feature_stage6.shape)\n",
    "# print(\"all_edge_index_stage6: \", all_edge_index_stage6.shape)\n",
    "# print(\"all_edge_weight_stage6: \", all_edge_weight_stage6.shape)\n",
    "# print(\"all_path_indices_stage6: \", all_path_indices_stage6.shape)\n",
    "# print(\"---------------------------------\")\n",
    "\n",
    "# # train test split 6 stage\n",
    "# train_delay_stage6, test_delay_stage6 = torch.utils.data.random_split(label_delay_stage6, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_slew_stage6, test_slew_stage6 = torch.utils.data.random_split(label_slew_stage6, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_node_feature_stage6, test_node_feature_stage6 = torch.utils.data.random_split(all_node_feature_stage6, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_path_feature_stage6, test_path_feature_stage6 = torch.utils.data.random_split(all_path_feature_stage6, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_edge_index_stage6, test_edge_index_stage6 = torch.utils.data.random_split(all_edge_index_stage6, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_edge_weight_stage6, test_edge_weight_stage6 = torch.utils.data.random_split(all_edge_weight_stage6, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_path_indices_stage6, test_path_indices_stage6 = torch.utils.data.random_split(all_path_indices_stage6, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# # save train test split 6 stage\n",
    "# torch.save(train_delay_stage6, os.path.join(TRAIN_DIR, \"train_delay_stage6.pt\"))\n",
    "# torch.save(train_slew_stage6, os.path.join(TRAIN_DIR, \"train_slew_stage6.pt\"))\n",
    "# torch.save(train_node_feature_stage6, os.path.join(TRAIN_DIR, \"train_node_feature_stage6.pt\"))\n",
    "# torch.save(train_path_feature_stage6, os.path.join(TRAIN_DIR, \"train_path_feature_stage6.pt\"))\n",
    "# torch.save(train_edge_index_stage6, os.path.join(TRAIN_DIR, \"train_edge_index_stage6.pt\"))\n",
    "# torch.save(train_edge_weight_stage6, os.path.join(TRAIN_DIR, \"train_edge_weight_stage6.pt\"))\n",
    "# torch.save(train_path_indices_stage6, os.path.join(TRAIN_DIR, \"train_path_indices_stage6.pt\"))\n",
    "# torch.save(test_delay_stage6, os.path.join(TEST_DIR, \"test_delay_stage6.pt\"))\n",
    "# torch.save(test_slew_stage6, os.path.join(TEST_DIR, \"test_slew_stage6.pt\"))\n",
    "# torch.save(test_node_feature_stage6, os.path.join(TEST_DIR, \"test_node_feature_stage6.pt\"))\n",
    "# torch.save(test_path_feature_stage6, os.path.join(TEST_DIR, \"test_path_feature_stage6.pt\"))\n",
    "# torch.save(test_edge_index_stage6, os.path.join(TEST_DIR, \"test_edge_index_stage6.pt\"))\n",
    "# torch.save(test_edge_weight_stage6, os.path.join(TEST_DIR, \"test_edge_weight_stage6.pt\"))\n",
    "# torch.save(test_path_indices_stage6, os.path.join(TEST_DIR, \"test_path_indices_stage6.pt\"))\n",
    "\n",
    "# # 4 stage\n",
    "# label_delay_stage4 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage4_label_delay.npy\"))\n",
    "# label_delay_stage4 = torch.from_numpy(label_delay_stage4)\n",
    "# label_delay_stage4 = label_delay_stage4.to(torch.float32).to(mps_device)\n",
    "# label_slew_stage4 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage4_label_slew.npy\"))\n",
    "# label_slew_stage4 = torch.from_numpy(label_slew_stage4)\n",
    "# label_slew_stage4 = label_slew_stage4.to(torch.float32).to(mps_device)\n",
    "# all_node_feature_stage4 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage4_node_features.npy\"))\n",
    "# all_node_feature_stage4 = torch.from_numpy(all_node_feature_stage4).to(torch.float32).to(mps_device)\n",
    "# all_path_feature_stage4 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage4_path_features.npy\"))\n",
    "# all_path_feature_stage4 = np.expand_dims(all_path_feature_stage4, axis=1)\n",
    "# all_path_feature_stage4 = torch.from_numpy(all_path_feature_stage4).to(torch.float32).to(mps_device)\n",
    "# all_edge_index_stage4 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage4_edge_index.npy\"))\n",
    "# all_edge_index_stage4 = torch.from_numpy(all_edge_index_stage4).to(torch.long).to(mps_device)\n",
    "# all_edge_weight_stage4 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage4_edge_weight.npy\"))\n",
    "# all_edge_weight_stage4 = torch.from_numpy(all_edge_weight_stage4).to(torch.float32).to(mps_device)\n",
    "# all_path_indices_stage4 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage4_path_indices.npy\"))\n",
    "# all_path_indices_stage4 = torch.from_numpy(all_path_indices_stage4).to(torch.long).to(mps_device)\n",
    "\n",
    "# print(\"label_delay_stage4: \", label_delay_stage4.shape)\n",
    "# print(\"label_slew_stage4: \", label_slew_stage4.shape)\n",
    "# print(\"all_node_feature_stage4: \", all_node_feature_stage4.shape)\n",
    "# print(\"all_path_feature_stage4: \", all_path_feature_stage4.shape)\n",
    "# print(\"all_edge_index_stage4: \", all_edge_index_stage4.shape)\n",
    "# print(\"all_edge_weight_stage4: \", all_edge_weight_stage4.shape)\n",
    "# print(\"all_path_indices_stage4: \", all_path_indices_stage4.shape)\n",
    "# print(\"---------------------------------\")\n",
    "\n",
    "# # train test split 4 stage\n",
    "# train_delay_stage4, test_delay_stage4 = torch.utils.data.random_split(label_delay_stage4, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_slew_stage4, test_slew_stage4 = torch.utils.data.random_split(label_slew_stage4, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_node_feature_stage4, test_node_feature_stage4 = torch.utils.data.random_split(all_node_feature_stage4, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_path_feature_stage4, test_path_feature_stage4 = torch.utils.data.random_split(all_path_feature_stage4, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_edge_index_stage4, test_edge_index_stage4 = torch.utils.data.random_split(all_edge_index_stage4, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_edge_weight_stage4, test_edge_weight_stage4 = torch.utils.data.random_split(all_edge_weight_stage4, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_path_indices_stage4, test_path_indices_stage4 = torch.utils.data.random_split(all_path_indices_stage4, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# # save train test split 4 stage\n",
    "# torch.save(train_delay_stage4, os.path.join(TRAIN_DIR, \"train_delay_stage4.pt\"))\n",
    "# torch.save(train_slew_stage4, os.path.join(TRAIN_DIR, \"train_slew_stage4.pt\"))\n",
    "# torch.save(train_node_feature_stage4, os.path.join(TRAIN_DIR, \"train_node_feature_stage4.pt\"))\n",
    "# torch.save(train_path_feature_stage4, os.path.join(TRAIN_DIR, \"train_path_feature_stage4.pt\"))\n",
    "# torch.save(train_edge_index_stage4, os.path.join(TRAIN_DIR, \"train_edge_index_stage4.pt\"))\n",
    "# torch.save(train_edge_weight_stage4, os.path.join(TRAIN_DIR, \"train_edge_weight_stage4.pt\"))\n",
    "# torch.save(train_path_indices_stage4, os.path.join(TRAIN_DIR, \"train_path_indices_stage4.pt\"))\n",
    "# torch.save(test_delay_stage4, os.path.join(TEST_DIR, \"test_delay_stage4.pt\"))\n",
    "# torch.save(test_slew_stage4, os.path.join(TEST_DIR, \"test_slew_stage4.pt\"))\n",
    "# torch.save(test_node_feature_stage4, os.path.join(TEST_DIR, \"test_node_feature_stage4.pt\"))\n",
    "# torch.save(test_path_feature_stage4, os.path.join(TEST_DIR, \"test_path_feature_stage4.pt\"))\n",
    "# torch.save(test_edge_index_stage4, os.path.join(TEST_DIR, \"test_edge_index_stage4.pt\"))\n",
    "# torch.save(test_edge_weight_stage4, os.path.join(TEST_DIR, \"test_edge_weight_stage4.pt\"))\n",
    "# torch.save(test_path_indices_stage4, os.path.join(TEST_DIR, \"test_path_indices_stage4.pt\"))\n",
    "\n",
    "# # 2 stage\n",
    "# label_delay_stage2 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage2_label_delay.npy\"))\n",
    "# label_delay_stage2 = torch.from_numpy(label_delay_stage2)\n",
    "# label_delay_stage2 = label_delay_stage2.to(torch.float32).to(mps_device)\n",
    "# label_slew_stage2 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage2_label_slew.npy\"))\n",
    "# label_slew_stage2 = torch.from_numpy(label_slew_stage2)\n",
    "# label_slew_stage2 = label_slew_stage2.to(torch.float32).to(mps_device)\n",
    "# all_node_feature_stage2 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage2_node_features.npy\"))\n",
    "# all_node_feature_stage2 = torch.from_numpy(all_node_feature_stage2).to(torch.float32).to(mps_device)\n",
    "# all_path_feature_stage2 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage2_path_features.npy\"))\n",
    "# all_path_feature_stage2 = np.expand_dims(all_path_feature_stage2, axis=1)\n",
    "# all_path_feature_stage2 = torch.from_numpy(all_path_feature_stage2).to(torch.float32).to(mps_device)\n",
    "# all_edge_index_stage2 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage2_edge_index.npy\"))\n",
    "# all_edge_index_stage2 = torch.from_numpy(all_edge_index_stage2).to(torch.long).to(mps_device)\n",
    "# all_edge_weight_stage2 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage2_edge_weight.npy\"))\n",
    "# all_edge_weight_stage2 = torch.from_numpy(all_edge_weight_stage2).to(torch.float32).to(mps_device)\n",
    "# all_path_indices_stage2 = np.load(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage2_path_indices.npy\"))\n",
    "# all_path_indices_stage2 = torch.from_numpy(all_path_indices_stage2).to(torch.long).to(mps_device)\n",
    "\n",
    "# print(\"label_delay_stage2: \", label_delay_stage2.shape)\n",
    "# print(\"label_slew_stage2: \", label_slew_stage2.shape)\n",
    "# print(\"all_node_feature_stage2: \", all_node_feature_stage2.shape)\n",
    "# print(\"all_path_feature_stage2: \", all_path_feature_stage2.shape)\n",
    "# print(\"all_edge_index_stage2: \", all_edge_index_stage2.shape)\n",
    "# print(\"all_edge_weight_stage2: \", all_edge_weight_stage2.shape)\n",
    "# print(\"all_path_indices_stage2: \", all_path_indices_stage2.shape)\n",
    "# print(\"---------------------------------\")\n",
    "\n",
    "# # train test split 2 stage\n",
    "# train_delay_stage2, test_delay_stage2 = torch.utils.data.random_split(label_delay_stage2, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_slew_stage2, test_slew_stage2 = torch.utils.data.random_split(label_slew_stage2, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_node_feature_stage2, test_node_feature_stage2 = torch.utils.data.random_split(all_node_feature_stage2, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_path_feature_stage2, test_path_feature_stage2 = torch.utils.data.random_split(all_path_feature_stage2, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_edge_index_stage2, test_edge_index_stage2 = torch.utils.data.random_split(all_edge_index_stage2, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_edge_weight_stage2, test_edge_weight_stage2 = torch.utils.data.random_split(all_edge_weight_stage2, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "# train_path_indices_stage2, test_path_indices_stage2 = torch.utils.data.random_split(all_path_indices_stage2, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# # save train test split 2 stage\n",
    "# torch.save(train_delay_stage2, os.path.join(TRAIN_DIR, \"train_delay_stage2.pt\"))\n",
    "# torch.save(train_slew_stage2, os.path.join(TRAIN_DIR, \"train_slew_stage2.pt\"))\n",
    "# torch.save(train_node_feature_stage2, os.path.join(TRAIN_DIR, \"train_node_feature_stage2.pt\"))\n",
    "# torch.save(train_path_feature_stage2, os.path.join(TRAIN_DIR, \"train_path_feature_stage2.pt\"))\n",
    "# torch.save(train_edge_index_stage2, os.path.join(TRAIN_DIR, \"train_edge_index_stage2.pt\"))\n",
    "# torch.save(train_edge_weight_stage2, os.path.join(TRAIN_DIR, \"train_edge_weight_stage2.pt\"))\n",
    "# torch.save(train_path_indices_stage2, os.path.join(TRAIN_DIR, \"train_path_indices_stage2.pt\"))\n",
    "# torch.save(test_delay_stage2, os.path.join(TEST_DIR, \"test_delay_stage2.pt\"))\n",
    "# torch.save(test_slew_stage2, os.path.join(TEST_DIR, \"test_slew_stage2.pt\"))\n",
    "# torch.save(test_node_feature_stage2, os.path.join(TEST_DIR, \"test_node_feature_stage2.pt\"))\n",
    "# torch.save(test_path_feature_stage2, os.path.join(TEST_DIR, \"test_path_feature_stage2.pt\"))\n",
    "# torch.save(test_edge_index_stage2, os.path.join(TEST_DIR, \"test_edge_index_stage2.pt\"))\n",
    "# torch.save(test_edge_weight_stage2, os.path.join(TEST_DIR, \"test_edge_weight_stage2.pt\"))\n",
    "# torch.save(test_path_indices_stage2, os.path.join(TEST_DIR, \"test_path_indices_stage2.pt\"))\n",
    "\n",
    "# # concat all samples\n",
    "# # label_delay = label_delay_stage2\n",
    "# # label_slew = label_slew_stage2\n",
    "# # all_node_feature = all_node_feature_stage2\n",
    "# # all_path_feature = all_node_feature_stage2\n",
    "# # all_edge_index = all_edge_index_stage2\n",
    "# # all_edge_weight = all_edge_weight_stage2\n",
    "# # all_path_indices = all_path_indices_stage2\n",
    "\n",
    "# # label_delay = torch.cat((label_delay_stage2, label_delay_stage4, label_delay_stage6, label_delay_stage8), dim=0)\n",
    "# # label_slew = torch.cat((label_slew_stage2, label_slew_stage4, label_slew_stage6, label_slew_stage8), dim=0)\n",
    "# # all_node_feature = torch.cat((all_node_feature_stage2, all_node_feature_stage4, all_node_feature_stage6, all_node_feature_stage8), dim=1)\n",
    "# # all_path_feature = torch.cat((all_path_feature_stage2, all_path_feature_stage4, all_path_feature_stage6, all_path_feature_stage8), dim=1)\n",
    "# # all_edge_index = torch.cat((all_edge_index_stage2, all_edge_index_stage4, all_edge_index_stage6, all_edge_index_stage8), dim=2)\n",
    "# # all_edge_weight = torch.cat((all_edge_weight_stage2, all_edge_weight_stage4, all_edge_weight_stage6, all_edge_weight_stage8), dim=1)\n",
    "# # all_path_indices = torch.cat((all_path_indices_stage2, all_path_indices_stage4, all_path_indices_stage6, all_path_indices_stage8), dim=2)\n",
    "\n",
    "# print(\"\\n\")\n",
    "# # print(\"label_delay Shape: \", label_delay.shape)\n",
    "# # print(\"label_slew Shape: \", label_slew.shape)\n",
    "# # print(\"node_feature Shape: \", all_node_feature.shape)\n",
    "# # print(\"path_feature Shape: \", all_path_feature.shape)\n",
    "# # print(\"edge_index Shape: \", all_edge_index.shape)\n",
    "# # print(\"edge_weight Shape: \", all_edge_weight.shape)\n",
    "# # print(\"path_indices Shape: \", all_path_indices.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SAMPLE_DIR = \"./sample/Data/\"\n",
    "TRAIN_DIR = \"./sample/Train/\"\n",
    "TEST_DIR = \"./sample/Test/\"\n",
    "\n",
    "label_delay = [\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_delay_stage2.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_delay_stage4.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_delay_stage6.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_delay_stage8.pt\"))])\n",
    "]\n",
    "label_slew = [\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_slew_stage2.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_slew_stage4.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_slew_stage6.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_slew_stage8.pt\"))])\n",
    "]\n",
    "all_node_feature = [\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_node_feature_stage2.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_node_feature_stage4.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_node_feature_stage6.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_node_feature_stage8.pt\"))])\n",
    "]\n",
    "all_path_feature = [\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_path_feature_stage2.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_path_feature_stage4.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_path_feature_stage6.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_path_feature_stage8.pt\"))])\n",
    "]\n",
    "all_edge_index = [\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_edge_index_stage2.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_edge_index_stage4.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_edge_index_stage6.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_edge_index_stage8.pt\"))])\n",
    "]\n",
    "all_edge_weight = [\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_edge_weight_stage2.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_edge_weight_stage4.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_edge_weight_stage6.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_edge_weight_stage8.pt\"))])\n",
    "]\n",
    "all_path_indices = [\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_path_indices_stage2.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_path_indices_stage4.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_path_indices_stage6.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TRAIN_DIR, \"train_path_indices_stage8.pt\"))])\n",
    "]\n",
    "\n",
    "num_samples = 8000\n",
    "# num_node = all_node_feature.shape[1]\n",
    "num_path = 1\n",
    "num_node_features = 9\n",
    "num_path_features = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_samples = label_delay.shape[0]\n",
    "# num_node = all_node_feature.shape[1]\n",
    "# num_path = 1\n",
    "# num_node_features = all_node_feature.shape[2]\n",
    "# num_path_features = all_path_feature.shape[2]\n",
    "\n",
    "# # print\n",
    "# print(\"num_node: \", num_node)\n",
    "# print(\"num_path: \", num_path)\n",
    "# print(\"num_node_features: \", num_node_features)\n",
    "# print(\"num_path_features: \", num_path_features)\n",
    "\n",
    "# # build a adjacency matrix that all the nodes are connected to the previous and next node\n",
    "# all_edge_index = []\n",
    "# all_edge_weight = []\n",
    "\n",
    "# # iterate through all the samples and stack them together\n",
    "# # edge_idx should be a 3D tensor of (10000, 2, 16)\n",
    "# # edge_weight should be a 2D tensor of (10000, 16)\n",
    "# for i in range(num_samples):\n",
    "#     M_adj = np.zeros((num_node, num_node))\n",
    "#     for j in range(num_node):\n",
    "#         if j == 0:\n",
    "#             M_adj[j][j + 1] = all_node_feature[i][j][7]\n",
    "#         elif j == num_node - 1:\n",
    "#             M_adj[j][j - 1] = all_node_feature[i][j][6]\n",
    "#         else:\n",
    "#             M_adj[j][j - 1] = all_node_feature[i][j][6]\n",
    "#             M_adj[j][j + 1] = all_node_feature[i][j][7]\n",
    "#     edge_index = torch.tensor(\n",
    "#         [[j, k] for j in range(num_node) for k in range(num_node) if M_adj[j, k] != 0],\n",
    "#         dtype=torch.long,\n",
    "#     ).t()\n",
    "#     edge_weight = torch.tensor(\n",
    "#         [\n",
    "#             M_adj[j, k]\n",
    "#             for j in range(num_node)\n",
    "#             for k in range(num_node)\n",
    "#             if M_adj[j, k] != 0\n",
    "#         ],\n",
    "#         dtype=torch.float,\n",
    "#     )\n",
    "#     all_edge_index.append(edge_index)\n",
    "#     all_edge_weight.append(edge_weight)\n",
    "\n",
    "# all_edge_index = torch.stack(all_edge_index).type(torch.int64).to(mps_device)\n",
    "# all_edge_weight = torch.stack(all_edge_weight).type(torch.float32).to(mps_device)\n",
    "\n",
    "# print(all_edge_index.shape)\n",
    "# print(all_edge_weight.shape)\n",
    "\n",
    "# # store as numpy array\n",
    "# np.save(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage2_all_edge_index.npy\"), all_edge_index.cpu().numpy())\n",
    "# np.save(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage2_all_edge_weight.npy\"), all_edge_weight.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # path_indices should be a 3D tensor of (10000, 1, 9) containing the indices of the nodes in the path\n",
    "# all_path_indices = np.zeros((num_samples, num_path, num_node))\n",
    "# for i in range(num_samples):\n",
    "#     all_path_indices[i][0] = np.arange(num_node)\n",
    "\n",
    "# print(all_path_indices.shape)\n",
    "# # to torch tensor with type int32\n",
    "# all_path_indices = torch.from_numpy(all_path_indices)\n",
    "# all_path_indices = all_path_indices.type(torch.int32).to(mps_device)\n",
    "\n",
    "# # store as numpy array\n",
    "# np.save(os.path.join(SAMPLE_DIR, \"NumPath10000_NumStage2_all_path_indices.npy\"), all_path_indices.cpu().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "GNN = GNNtrans(num_node_features, num_path_features)\n",
    "# GNN.load_state_dict(torch.load(\"GNNtrans.pt\"))\n",
    "GNN.to(mps_device)\n",
    "optimizer = torch.optim.Adam(GNN.parameters(), lr=1e-4)\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "# for epoch in range(20):\n",
    "#     epoch_loss = 0\n",
    "#     for stage_idx in [0, 1, 2, 3]:\n",
    "#     # for stage_idx in [3]:\n",
    "#         for sample_idx, (node_feature, edge_idx, edge_weight, path_idx, path_feature, slew_target, delay_target) in enumerate(\n",
    "#             zip(\n",
    "#                 all_node_feature[stage_idx],\n",
    "#                 all_edge_index[stage_idx],\n",
    "#                 all_edge_weight[stage_idx],\n",
    "#                 all_path_indices[stage_idx],\n",
    "#                 all_path_feature[stage_idx],\n",
    "#                 label_slew[stage_idx],\n",
    "#                 label_delay[stage_idx]\n",
    "#             )\n",
    "#         ):\n",
    "#             optimizer.zero_grad()\n",
    "#             slew, delay = GNN(node_feature, edge_idx, edge_weight, path_idx, path_feature)\n",
    "#             loss = F.mse_loss(slew, slew_target) + F.mse_loss(delay, delay_target)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             epoch_loss += loss.item()\n",
    "\n",
    "#     loss_history.append(epoch_loss / num_samples)\n",
    "\n",
    "#     print(\"Epoch: \", epoch, \" Avg Loss: \", epoch_loss / num_samples)\n",
    "\n",
    "# shuffle the training data but generate stage_idx and sample_idx pair randomly\n",
    "stage_idx = np.random.randint(0, 4, num_samples)\n",
    "sample_idx = np.random.randint(0, 8000, num_samples)\n",
    "\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0\n",
    "    for i in range(num_samples):\n",
    "        optimizer.zero_grad()\n",
    "        slew, delay = GNN(\n",
    "            all_node_feature[stage_idx[i]][sample_idx[i]],\n",
    "            all_edge_index[stage_idx[i]][sample_idx[i]],\n",
    "            all_edge_weight[stage_idx[i]][sample_idx[i]],\n",
    "            all_path_indices[stage_idx[i]][sample_idx[i]],\n",
    "            all_path_feature[stage_idx[i]][sample_idx[i]],\n",
    "        )\n",
    "        loss = F.mse_loss(slew, label_slew[stage_idx[i]][sample_idx[i]]) + F.mse_loss(\n",
    "            delay, label_delay[stage_idx[i]][sample_idx[i]]\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    loss_history.append(epoch_loss / num_samples)\n",
    "\n",
    "    print(\"Epoch: \", epoch, \" Avg Loss: \", epoch_loss / num_samples)\n",
    "\n",
    "# save the model\n",
    "torch.save(GNN.state_dict(), \"GNNtrans.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "# plot the loss history\n",
    "# normalize the loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss History\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differences in input i = 0 and i = 1\n",
    "print(all_node_feature[0][0])\n",
    "print(all_node_feature[1][0])\n",
    "print(all_node_feature[0][1])\n",
    "print(all_node_feature[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "test_label_delay = [\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_delay_stage2.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_delay_stage4.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_delay_stage6.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_delay_stage8.pt\"))])\n",
    "]\n",
    "\n",
    "test_label_slew = [\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_slew_stage2.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_slew_stage4.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_slew_stage6.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_slew_stage8.pt\"))])\n",
    "]\n",
    "\n",
    "test_node_feature = [\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_node_feature_stage2.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_node_feature_stage4.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_node_feature_stage6.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_node_feature_stage8.pt\"))])\n",
    "]\n",
    "\n",
    "test_edge_index = [\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_edge_index_stage2.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_edge_index_stage4.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_edge_index_stage6.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_edge_index_stage8.pt\"))])\n",
    "]\n",
    "\n",
    "test_edge_weight = [\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_edge_weight_stage2.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_edge_weight_stage4.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_edge_weight_stage6.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_edge_weight_stage8.pt\"))])\n",
    "]\n",
    "\n",
    "test_path_indices = [\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_path_indices_stage2.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_path_indices_stage4.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_path_indices_stage6.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_path_indices_stage8.pt\"))])\n",
    "]\n",
    "\n",
    "test_path_feature = [\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_path_feature_stage2.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_path_feature_stage4.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_path_feature_stage6.pt\"))]),\n",
    "    torch.stack([t for t in torch.load(os.path.join(TEST_DIR, \"test_path_feature_stage8.pt\"))])\n",
    "]\n",
    "\n",
    "# calculate the R2 score\n",
    "pred_slew = []\n",
    "pred_delay = []\n",
    "target_slew = []\n",
    "target_delay = []\n",
    "\n",
    "stage_id = 3\n",
    "\n",
    "for i in range(len(test_label_delay[stage_id])):\n",
    "    node_feature = test_node_feature[stage_id][i]\n",
    "    edge_idx = test_edge_index[stage_id][i]\n",
    "    edge_weight = test_edge_weight[stage_id][i]\n",
    "    path_idx = test_path_indices[stage_id][i]\n",
    "    path_feature = test_path_feature[stage_id][i]\n",
    "    slew_target = test_label_slew[stage_id][i]\n",
    "    delay_target = test_label_delay[stage_id][i]\n",
    "\n",
    "    slew, delay = GNN(node_feature, edge_idx, edge_weight, path_idx, path_feature)\n",
    "    pred_slew.append(slew.item())\n",
    "    pred_delay.append(delay.item())\n",
    "    target_slew.append(slew_target.item())\n",
    "    target_delay.append(delay_target.item())\n",
    "\n",
    "print(\"predicted slew: \", pred_slew)\n",
    "print(\"target slew: \", target_slew)\n",
    "print(\"predicted delay: \", pred_delay)\n",
    "print(\"target delay: \", target_delay)\n",
    "\n",
    "print(\"R2 score for slew: \", r2_score(target_slew, pred_slew))\n",
    "print(\"R2 score for delay: \", r2_score(target_delay, pred_delay))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- Base : https://colab.research.google.com/github/VisiumCH/AMLD-2021-Graphs/blob/master/notebooks/workshop_notebook.ipynb#scrollTo=7kPXvM8OxMJS\n",
    "- GraphSAGE : https://colab.research.google.com/drive/1udeUfWJzvMlLO7sGUDGsHo8cRPMicajl?usp=sharing#scrollTo=wTR4wQG31Vtk\n",
    "- GAT : https://colab.research.google.com/github/PytorchLightning/lightning-tutorials/blob/publication/.notebooks/course_UvA-DL/06-graph-neural-networks.ipynb#scrollTo=6c42fc29"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
